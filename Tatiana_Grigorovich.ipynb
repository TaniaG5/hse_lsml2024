{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a93924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/21 11:07:16 WARN Utils: Your hostname, Tatianas-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.15 instead (on interface en0)\n",
      "24/01/21 11:07:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/21 11:07:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName='sgatg')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "LIMIT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc83cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\tsession_id\tevent_type\tevent_page\ttimestamp\r\n",
      "0\t874\tpage\tmain\t1696371064\r\n",
      "0\t874\tevent\tmain\t1696372696\r\n",
      "0\t874\tevent\tmain\t1696373564\r\n",
      "0\t874\tpage\trabota\t1696374894\r\n",
      "0\t874\tevent\trabota\t1696377393\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 6 clickstream.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601534fd",
   "metadata": {},
   "source": [
    "# SQL Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53d0901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|route                |count|\n",
      "+---------------------+-----+\n",
      "|main                 |8184 |\n",
      "|main-archive         |1113 |\n",
      "|main-rabota          |1047 |\n",
      "|main-internet        |897  |\n",
      "|main-bonus           |870  |\n",
      "|main-news            |769  |\n",
      "|main-tariffs         |677  |\n",
      "|main-online          |587  |\n",
      "|main-vklad           |518  |\n",
      "|main-rabota-archive  |170  |\n",
      "|main-archive-rabota  |167  |\n",
      "|main-bonus-archive   |143  |\n",
      "|main-rabota-bonus    |139  |\n",
      "|main-news-rabota     |135  |\n",
      "|main-bonus-rabota    |135  |\n",
      "|main-archive-internet|132  |\n",
      "|main-rabota-news     |130  |\n",
      "|main-internet-rabota |129  |\n",
      "|main-archive-news    |126  |\n",
      "|main-rabota-internet |124  |\n",
      "|main-internet-archive|123  |\n",
      "|main-archive-bonus   |117  |\n",
      "|main-internet-bonus  |115  |\n",
      "|main-tariffs-internet|114  |\n",
      "|main-news-archive    |113  |\n",
      "|main-news-internet   |109  |\n",
      "|main-archive-tariffs |104  |\n",
      "|main-internet-news   |103  |\n",
      "|main-tariffs-archive |103  |\n",
      "|main-rabota-main     |94   |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.option(\"header\", True).csv(\"clickstream.csv\", sep = '\\t')\n",
    "data.createOrReplaceTempView(\"clickstream\")\n",
    "\n",
    "sql_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    concat_ws(\"-\", route_list) as route, count(*) as count\n",
    "FROM\n",
    "(\n",
    "    SELECT collect_list(event_page) as route_list\n",
    "    FROM\n",
    "    (\n",
    "        SELECT\n",
    "            user_id, session_id, event_page,\n",
    "            lag(event_page) OVER(PARTITION BY user_id, session_id ORDER BY timestamp) as lag_page,\n",
    "            any(contains(event_type,'error')) OVER(PARTITION BY user_id, session_id ORDER BY timestamp) as prev\n",
    "        FROM clickstream\n",
    "        ORDER BY user_id, session_id, timestamp\n",
    "    )\n",
    "    WHERE !prev and ((event_page != lag_page) OR isnull(lag_page))\n",
    "    GROUP BY user_id, session_id\n",
    ")\n",
    "GROUP BY route_list\n",
    "ORDER by count DESC\n",
    "LIMIT \"\"\" + str(LIMIT))\n",
    "sql_result.write.option(\"sep\",\"\\t\").mode('overwrite').csv('sql_results')\n",
    "\n",
    "sql_result.show(n=LIMIT, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4219fc4",
   "metadata": {},
   "source": [
    "# RDD Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a40935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\t8185\n",
      "main-archive\t1113\n",
      "main-rabota\t1047\n",
      "main-internet\t897\n",
      "main-bonus\t870\n",
      "main-news\t769\n",
      "main-tariffs\t677\n",
      "main-online\t587\n",
      "main-vklad\t518\n",
      "main-rabota-archive\t170\n",
      "main-archive-rabota\t167\n",
      "main-bonus-archive\t143\n",
      "main-rabota-bonus\t139\n",
      "main-bonus-rabota\t135\n",
      "main-news-rabota\t135\n",
      "main-archive-internet\t132\n",
      "main-rabota-news\t130\n",
      "main-internet-rabota\t129\n",
      "main-archive-news\t126\n",
      "main-rabota-internet\t124\n",
      "main-internet-archive\t123\n",
      "main-archive-bonus\t117\n",
      "main-internet-bonus\t115\n",
      "main-tariffs-internet\t113\n",
      "main-news-archive\t113\n",
      "main-news-internet\t109\n",
      "main-archive-tariffs\t104\n",
      "main-internet-news\t103\n",
      "main-tariffs-archive\t103\n",
      "main-rabota-main\t94\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(\"clickstream.csv\")  # we do not drop header line, but it will meet just 1 time\n",
    "                                                      # and won't be in most popular routes\n",
    "rdd = rdd.map(lambda line: line.split(\"\\t\"))\n",
    "rdd = rdd.map(lambda x: (x[0] + '-' + x[1], 'error' if x[2].find('error') != -1 else x[3])) # make \n",
    "                                        # user_id-session_id string as a key and page or 'error' as value\n",
    "                                        # 'error' we write only for pages on which event contained word 'error'\n",
    "rdd = rdd.groupByKey()  # create list of pages or errors if event contained word 'error' for every user-session id\n",
    "\n",
    "def drop_starting_from_error(line):\n",
    "    user_session, pages = line\n",
    "    correct_pages = []\n",
    "    prev_page = None\n",
    "    for page in pages:\n",
    "        if page == 'error':\n",
    "            break\n",
    "        if page != prev_page:\n",
    "            correct_pages.append(page)\n",
    "            prev_page = page\n",
    "#     if '-'.join(correct_pages) == 'main-internet-tariffs-main':\n",
    "#         print(user_session)\n",
    "    return ('-'.join(correct_pages), user_session)\n",
    "\n",
    "rdd = rdd.map(drop_starting_from_error)  # drop all pages starting from the one on which error occured\n",
    "                                         # return route and user-session ids\n",
    "rdd = rdd.groupByKey()  # create list of user_session ids for every route\n",
    "rdd = rdd.map(lambda x: (x[0], len(set(x[1]))))\n",
    "rdd = rdd.sortBy(lambda x: -x[1])\n",
    "rdd = rdd.map(lambda x: x[0] + '\\t' + str(x[1]))\n",
    "with open('rdd_results.csv', 'w') as file:\n",
    "    file.write('\\n'.join(rdd.take(LIMIT)))\n",
    "for line in rdd.take(LIMIT):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ca79c",
   "metadata": {},
   "source": [
    "# DF Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b55d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+----------+\n",
      "|user_id|session_id|event_type|event_page| timestamp|\n",
      "+-------+----------+----------+----------+----------+\n",
      "|      0|       874|      page|      main|1696371064|\n",
      "|      0|       874|     event|      main|1696372696|\n",
      "|      0|       874|     event|      main|1696373564|\n",
      "|      0|       874|      page|    rabota|1696374894|\n",
      "|      0|       874|     event|    rabota|1696377393|\n",
      "|      0|       874|      page|    online|1696378229|\n",
      "+-------+----------+----------+----------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, when, udf, collect_list\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", True).csv(\"clickstream.csv\")\n",
    "# df.event_page = df\n",
    "df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9bf9988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+\n",
      "|user_id|session_id|event_page| timestamp|\n",
      "+-------+----------+----------+----------+\n",
      "|      0|       874|      main|1696371064|\n",
      "|      0|       874|      main|1696372696|\n",
      "|      0|       874|      main|1696373564|\n",
      "|      0|       874|    rabota|1696374894|\n",
      "|      0|       874|    rabota|1696377393|\n",
      "|      0|       874|    online|1696378229|\n",
      "+-------+----------+----------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small = df.withColumn(\"event_page\", when(df['event_type'].contains('error'), 'error')\\\n",
    "              .otherwise(df[\"event_page\"]))[['user_id', 'session_id', 'event_page', 'timestamp']]\n",
    "df_small.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d28ad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               route|\n",
      "+--------------------+\n",
      "|              [main]|\n",
      "|              [main]|\n",
      "|[main, internet, ...|\n",
      "|[main, archive, e...|\n",
      "|[main, main, main...|\n",
      "|[main, news, news...|\n",
      "+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pages_w_errors = df_small.groupby(['user_id', 'session_id'])\\\n",
    "           .agg(collect_list(df_small.event_page)\\\n",
    "           .alias('route'))[['route']]\n",
    "df_pages_w_errors.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9843bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               route|\n",
      "+--------------------+\n",
      "|                main|\n",
      "|                main|\n",
      "|main-internet-arc...|\n",
      "|        main-archive|\n",
      "|main-bonus-intern...|\n",
      "|main-news-interne...|\n",
      "+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def route_from_pages(pages):\n",
    "    route = []\n",
    "    prev_page = None\n",
    "    for page in pages:\n",
    "        if page == 'error':\n",
    "            break\n",
    "        if page != prev_page:\n",
    "            route.append(page)\n",
    "            prev_page = page\n",
    "    return '-'.join(route)\n",
    "\n",
    "udf_route_from_pages = udf(route_from_pages, StringType())\n",
    "\n",
    "df_routes = df_pages_w_errors.withColumn('route', udf_route_from_pages('route'))\n",
    "\n",
    "df_routes.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b62a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8185|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  896|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  677|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  139|\n",
      "|   main-bonus-rabota|  135|\n",
      "|    main-news-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  124|\n",
      "|main-internet-arc...|  123|\n",
      "|  main-archive-bonus|  117|\n",
      "| main-internet-bonus|  115|\n",
      "|main-tariffs-inte...|  113|\n",
      "|   main-news-archive|  113|\n",
      "|  main-news-internet|  109|\n",
      "|main-archive-tariffs|  104|\n",
      "|  main-internet-news|  103|\n",
      "|main-tariffs-archive|  103|\n",
      "|    main-rabota-main|   94|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/21 11:07:30 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "df_routes_w_counts = df_routes.groupBy('route').count().sort('count', ascending=[False]).limit(LIMIT)\n",
    "\n",
    "df_routes_w_counts.write.option(\"sep\",\"\\t\").mode('overwrite').csv('df_results')\n",
    "\n",
    "df_routes_w_counts.show(LIMIT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
